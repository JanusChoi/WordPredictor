---
title: "EDA_SwiftKey_enUS"
author: "YingCai"
date: "2018-5-7"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Introduction

The goal of this project is just to display that you've gotten used to working with the data and that you are on track to create your prediction algorithm. Please submit a report on R Pubs (http://rpubs.com/) that **explains your exploratory analysis** and **your goals for the eventual app and algorithm**. This document should be concise and explain only the major features of the data you have identified and briefly summarize your plans for creating the prediction algorithm and Shiny app in a way that would be understandable to a non-data scientist manager. You should make use of tables and plots to illustrate important summaries of the data set. 

The motivation for this project is to: 

1. Demonstrate that you've downloaded the data and have successfully loaded it in.
2. Create a basic report of summary statistics about the data sets.
3. Report any interesting findings that you amassed so far.
4. Get feedback on your plans for creating a prediction algorithm and Shiny app.

And the following plan for this project will be : build a shiny app which can produce an article automatically base on some input words or topics.

# 2. Load Data & Sampling

```{r, echo=FALSE}
#### Load necessay libraries
library(dplyr)
library(ggplot2)
library(LaF)
library(quanteda)
library(RColorBrewer)
library(tm)
library(wordcloud)

#### Get data in directory & load sample data for analyse
setwd("E:/W4Wise/10_Projects/P06_DS_Capstone/final/en_US")
set.seed(6000)

#### Consider limited memories, Taking sample from data as
#### blogs   5000
#### news    5000
#### twitter 5000

con <- file("en_US.blogs.txt")
data_blogs <- readLines(con, encoding = "UTF-8", skipNul = TRUE)
db_filesize <- file.size("en_US.blogs.txt")
db_wordsCnt <- nchar(data_blogs)
db_tmax <- which.max(db_wordsCnt)
db_longestWordCnt <- nchar(data_blogs[db_tmax])

con <- file("en_US.news.txt")
data_news <- readLines(con, encoding = "UTF-8", skipNul = TRUE, warn = FALSE)
dn_filesize <- file.size("en_US.news.txt")
dn_wordsCnt <- nchar(data_news)
dn_tmax <- which.max(dn_wordsCnt)
dn_longestWordCnt <- nchar(data_news[dn_tmax])


con <- file("en_US.twitter.txt")
data_twitter <- readLines(con, encoding = "UTF-8", skipNul = TRUE)
dt_filesize <- file.size("en_US.twitter.txt")
dt_wordsCnt <- nchar(data_twitter)
dt_tmax <- which.max(dt_wordsCnt)
dt_longestWordCnt <- nchar(data_twitter[dt_tmax])

dataframe.blogs <- c(db_filesize, length(db_wordsCnt), db_longestWordCnt)
dataframe.news <- c(dn_filesize, length(dn_wordsCnt), dn_longestWordCnt)
dataframe.twitter <- c(dt_filesize, length(dt_wordsCnt), dt_longestWordCnt)

info <- data.frame(rbind(dataframe.blogs, dataframe.news, dataframe.twitter))
names(info) <- c("File Size(MB)", "Word Count", "Longest Line")
row.names(info) <- c("Blogs", "News", "Twitter")

info

```

# 3. Basics about the Data
## 3.1 Data Preprocessing

```{r}
#### sampling
data_blogs <- sample(data_blogs, 5000)
data_news <- sample(data_news, 5000)
data_twitter <- sample(data_twitter, 10000)

data_all <- c(data_blogs, data_news, data_twitter)

#### data cleaning

data_cls <- gsub("[^-[:alpha:]:, ]", "", data_all)
data_cls <- tolower(data_cls)
data_cls <- gsub("-"," ",data_cls)
data_cls <- gsub(":"," ",data_cls)
data_cls <- gsub(","," ",data_cls)

#save(data_cls, file = "datall.RData")
#corpus <- Corpus(VectorSource(data_cls))
#corpus <- tm_map(corpus, content_transformer(removePunctuation), lazy = TRUE)
#corpus <- tm_map(corpus, content_transformer(removeNumbers), lazy = TRUE)
#corpus <- tm_map(corpus, content_transformer(tolower))
#corpus <- tm_map(corpus, content_transformer(stripWhitespace))
#corpus <- tm_map(corpus, content_transformer(PlainTextDocument), lazy = TRUE)

#save(corpus, file="WorkingCorpus.RData")
Trim <-
function (x) gsub("^\\s+|\\s+$", "", x)

rm_stopwords <-
function (text.var, stopwords = qdapDictionaries::Top25Words, unlist = FALSE, 
    separate = TRUE,   char.keep = NULL, 
    names = FALSE, ignore.case = TRUE, apostrophe.remove = FALSE, ...) {
    Stopwords <- if (is.null(stopwords)) {
        c(" ")
    } else {
        stopwords
    }
    SW <- function(text.var, stopwords) {
        "%w/o%" <- function(x, y) x[!x %in% y]
        breaker2 <- function(X) {
            strsplit(X, "[[:space:]]", perl=TRUE)
        }  
            unlist(breaker2(Trim(text.var))) %w/o% stopwords
    }
    x <- lapply(text.var, function(x) SW(x, Stopwords))
        x <- sapply(x, paste, collapse = " ", USE.NAMES = FALSE)
       
    return(x)
}

list_stopwords<- c(tolower(Trim(stopwords('english'))),"im","u","youre","d","ive")
datacls_rm_stopwords <- rm_stopwords(data_cls, stopwords = list_stopwords, separate=FALSE)
```

## 3.2 Exploratory Analysis

### 3.2.1 Create N-Gram Functions
```{r}
#### N-Gram Tokenizer
GenerateNGram <-function(Words, nGram) {
bigrams <- dfm(Words,ngrams = nGram,  removeNumbers = TRUE,  removePunct = TRUE, removeSeparators = TRUE)
top_bi <- data.frame(topfeatures(bigrams, n=10000))
top_bi <- data.frame(Words=rownames(top_bi), Freq=top_bi)      
names(top_bi) <- c("words", "freq")
top_bi$words <- factor(top_bi$words,levels = top_bi$words[order(top_bi$freq, decreasing = TRUE)])              
top_bi
}

#### function for drawing Histogram
drawHistogram<-function(resultDf,displayItems,title){
  ggplot(resultDf[1:displayItems,], aes(reorder(words, freq), freq)) +
         labs(x = "Words/Phrases", y = "Frequency") + ggtitle(title) +
         theme(axis.text.x = element_text(angle = 90, size = 10, vjust = 0.5)) +
         geom_bar(stat = "identity") +  coord_flip()
  
}

#### function for drawing Cloud map
drawCloud<-function(resultDf,displayItems,size){
pal2 <- brewer.pal(8,"Dark2")
wordcloud(resultDf$words, resultDf$freq,scale=c(size,.2), max.words=displayItems, random.order=FALSE, rot.per=.15, colors=pal2)
}
```

1. Unigram

```{r}
gen_1gram <- GenerateNGram(data_cls, 1)
drawHistogram(gen_1gram, 30, "Unigram for all Data")
```

```{r}
drawCloud(gen_1gram,100,3)
```

```{r}
gen_1gram_rm_stopwords <- GenerateNGram(datacls_rm_stopwords, 1)
drawHistogram(gen_1gram_rm_stopwords, 30, "Unigram for without Stopwords")
```

```{r}
drawCloud(gen_1gram_rm_stopwords,100,3)
```

2. BiGram

```{r}
gen_2gram<-GenerateNGram(data_cls,2)
drawHistogram(gen_2gram,30,"Bigram for all data")
```

```{r}
drawCloud(gen_2gram,100,3)
```

```{r}
gen_2gram_rm_stopwords<-GenerateNGram(datacls_rm_stopwords,2)
drawHistogram(gen_2gram_rm_stopwords,30,"Bigram without stopwords")
```

```{r}
drawCloud(gen_2gram_rm_stopwords,100,3)
```


## 4. Following Plans

Build a shiny app which can produce an article automatically base on some input words or topics.